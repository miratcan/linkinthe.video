{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type># linkinthe.video - Open Source Pipeline\n\n**Modeller:**\n- Whisper - Transcription (fallback)\n- Qwen2 7B - ÃœrÃ¼n Ã§Ä±karma\n- LLaVA 1.6 - Frame analizi\n\n**AkÄ±ÅŸ:**\n1. Transcript al (YouTube altyazÄ±sÄ± veya Whisper)\n2. LLM ile Ã¼rÃ¼n Ã§Ä±kar â†’ found[] + lost[]\n3. Lost varsa â†’ Video indir â†’ Frame Ã§Ä±kar â†’ Vision ile tanÄ±\n\n**Lazy Evaluation:** Video sadece gerektiÄŸinde indirilir.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 0. Kurulum\n\n**Ã–NEMLÄ°:** Runtime â†’ Change runtime type â†’ T4 GPU seÃ§!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GPU kontrol\n!nvidia-smi\n\nimport torch\nif torch.cuda.is_available():\n    print(f\"\\nâœ… GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"âŒ GPU YOK! Runtime â†’ Change runtime type â†’ T4 GPU seÃ§!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Paketleri kur\n!pip install -q youtube-transcript-api openai-whisper\n!pip install -q transformers accelerate bitsandbytes sentencepiece protobuf\n\nprint(\"âœ… Kurulum tamamlandÄ±!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport time\nimport requests\n\n# Ã‡alÄ±ÅŸma klasÃ¶rÃ¼\nWORK_DIR = \"/content/linkinthe_test\"\nos.makedirs(WORK_DIR, exist_ok=True)\n\n# Test videosu\nVIDEO_URL = \"https://www.youtube.com/watch?v=WlgjElhWD-U\"\nVIDEO_ID = VIDEO_URL.split(\"v=\")[-1].split(\"&\")[0]\n\n# State\nlocal_video_path = None\ntranscript_segments = None  # timestamp'li\ntranscript_text = None\n\nprint(f\"Video ID: {VIDEO_ID}\")\nprint(f\"Video URL: {VIDEO_URL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n## 1. Transcript Al"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from youtube_transcript_api import YouTubeTranscriptApi\n\n# Ã–nce YouTube altyazÄ±sÄ±nÄ± dene\ntry:\n    transcript_segments = YouTubeTranscriptApi.get_transcript(VIDEO_ID, languages=['tr', 'en'])\n    transcript_text = \" \".join([s['text'] for s in transcript_segments])\n    print(f\"âœ… YouTube altyazÄ±sÄ± bulundu! ({len(transcript_text)} karakter)\")\n    print(f\"   {len(transcript_segments)} segment (timestamp'li)\")\n    print(\"\\n\" + \"=\"*60)\n    print(transcript_text[:1000])\n    print(\"=\"*60)\nexcept Exception as e:\n    print(f\"âŒ YouTube altyazÄ±sÄ± yok: {e}\")\n    print(\"â†’ Video indirip Whisper kullanacaÄŸÄ±z...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n## 2. Video YÃ¼kle (sadece altyazÄ± yoksa)\n\nYouTube altyazÄ±sÄ± varsa bu adÄ±mÄ± **ATLA**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Manuel video upload\nfrom google.colab import files\n\nlocal_video_path = f\"{WORK_DIR}/video.mp4\"\n\nprint(\"ğŸ“ Video dosyasÄ±nÄ± yÃ¼kle (.mp4)\")\nuploaded = files.upload()\n\nif uploaded:\n    filename = list(uploaded.keys())[0]\n    with open(local_video_path, 'wb') as f:\n        f.write(uploaded[filename])\n    size_mb = os.path.getsize(local_video_path) / (1024 * 1024)\n    print(f\"âœ… Video yÃ¼klendi: {size_mb:.1f} MB\")"
  },
  {
   "cell_type": "code",
   "source": "# Transcript yoksa â†’ indir + Whisper\nif not transcript_text:\n    local_video_path = download_video(VIDEO_URL, VIDEO_ID)\n    \n    if local_video_path:\n        # Ses Ã§Ä±kar\n        audio_path = f\"{WORK_DIR}/audio.wav\"\n        !ffmpeg -i \"{local_video_path}\" -vn -acodec pcm_s16le -ar 16000 -ac 1 \"{audio_path}\" -y -loglevel error\n        print(f\"âœ… Ses Ã§Ä±karÄ±ldÄ±\")\n        \n        # Whisper\n        import whisper\n        print(\"Whisper yÃ¼kleniyor...\")\n        model = whisper.load_model(\"medium\")\n        \n        print(\"Transkript alÄ±nÄ±yor...\")\n        result = model.transcribe(audio_path)\n        \n        transcript_text = result['text']\n        transcript_segments = result['segments']\n        print(f\"âœ… Whisper tamamlandÄ±! ({len(transcript_text)} karakter)\")\n        \n        del model\n        torch.cuda.empty_cache()\nelse:\n    print(\"â­ï¸ YouTube altyazÄ±sÄ± var, video indirmeye gerek yok.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n## 3. LLM ile ÃœrÃ¼n Ã‡Ä±kar (found/lost)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Qwen2 7B - tamamen aÃ§Ä±k, Ã§ok iyi performans\nMODEL_ID = \"Qwen/Qwen2-7B-Instruct\"\n\nprint(\"Qwen2 7B yÃ¼kleniyor...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\nllm_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\nprint(\"âœ… Qwen2 yÃ¼klendi!\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def ask_llama(prompt, max_tokens=1500):\n    messages = [\n        {\"role\": \"system\", \"content\": \"Sen bir Ã¼rÃ¼n tespit asistanÄ±sÄ±n. JSON formatÄ±nda cevap ver.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(llm_model.device)\n    outputs = llm_model.generate(input_ids, max_new_tokens=max_tokens, do_sample=True, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n\n# ÃœrÃ¼n Ã§Ä±karma prompt'u - found/lost ayÄ±rÄ±yor\nPRODUCT_PROMPT = f\"\"\"Bu bir YouTube videosunun transkripti. Videoda bahsedilen Ã¼rÃ¼nleri bul.\n\nKURALLAR:\n1. Emin olduÄŸun Ã¼rÃ¼nler (marka+model belli) â†’ \"found\" listesine\n2. Belirsiz olanlar (sadece \"kablo\", \"tripod\" gibi) â†’ \"lost\" listesine  \n3. Duplicate ekleme, aynÄ± Ã¼rÃ¼nÃ¼ bir kere yaz\n4. Her Ã¼rÃ¼n iÃ§in timestamp (saniye) ver\n\nJSON formatÄ±:\n{{\n  \"found\": [{{\"name\": \"Sony A7IV\", \"category\": \"kamera\", \"timestamp\": 125}}],\n  \"lost\": [{{\"name\": \"kablo\", \"category\": \"aksesuar\", \"timestamp\": 340}}]\n}}\n\nTranskript:\n---\n{transcript_text[:6000]}\n---\n\nSADECE JSON dÃ¶n.\"\"\"\n\nprint(\"ÃœrÃ¼nler Ã§Ä±karÄ±lÄ±yor...\")\nstart = time.time()\nllm_response = ask_llama(PRODUCT_PROMPT)\nprint(f\"âœ… TamamlandÄ±! ({time.time()-start:.1f} sn)\")\nprint(llm_response)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# JSON parse et\ndef parse_llm_json(text):\n    clean = text\n    if \"```json\" in clean:\n        clean = clean.split(\"```json\")[1].split(\"```\")[0]\n    elif \"```\" in clean:\n        clean = clean.split(\"```\")[1].split(\"```\")[0]\n    start_idx = clean.find(\"{\")\n    end_idx = clean.rfind(\"}\") + 1\n    if start_idx != -1 and end_idx > start_idx:\n        clean = clean[start_idx:end_idx]\n    return json.loads(clean)\n\ntry:\n    data = parse_llm_json(llm_response)\n    found = data.get(\"found\", [])\n    lost = data.get(\"lost\", [])\n    \n    print(f\"âœ… FOUND ({len(found)} Ã¼rÃ¼n):\")\n    for p in found:\n        print(f\"   ğŸŸ¢ {p['name']} ({p.get('category', '?')}) @ {p.get('timestamp', '?')}s\")\n    \n    print(f\"\\nâš ï¸ LOST ({len(lost)} Ã¼rÃ¼n) - Vision ile Ã§Ã¶zÃ¼lecek:\")\n    for p in lost:\n        print(f\"   ğŸŸ¡ {p['name']} ({p.get('category', '?')}) @ {p.get('timestamp', '?')}s\")\nexcept Exception as e:\n    print(f\"âŒ Parse hatasÄ±: {e}\")\n    found, lost = [], []\n\n# Llama'yÄ± temizle\ndel llm_model, tokenizer\ntorch.cuda.empty_cache()\nprint(\"\\nâœ… Llama bellekten silindi.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<cell_type>markdown</cell_type>---\n## 4. Lost ÃœrÃ¼nleri Vision ile Ã‡Ã¶z\n\nVideo sadece lost varsa indirilir."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not lost:\n    print(\"âœ… Lost yok, Vision'a gerek yok!\")\n    vision_model = None\nelse:\n    # Video indir (henÃ¼z indirilmediyse)\n    if not local_video_path:\n        print(\"Lost var, video indiriliyor...\")\n        local_video_path = download_video(VIDEO_URL, VIDEO_ID)\n    \n    if local_video_path:\n        # LLaVA yÃ¼kle\n        from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n        from PIL import Image as PILImage\n        \n        print(\"LLaVA yÃ¼kleniyor...\")\n        VISION_MODEL = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n        vision_processor = LlavaNextProcessor.from_pretrained(VISION_MODEL)\n        vision_model = LlavaNextForConditionalGeneration.from_pretrained(\n            VISION_MODEL,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n            load_in_4bit=True,\n        )\n        print(\"âœ… LLaVA yÃ¼klendi!\")\n    else:\n        print(\"âŒ Video indirilemedi, Vision atlanÄ±yor\")\n        vision_model = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_frame(video_path, timestamp_sec):\n    \"\"\"Video'dan belirli saniyede frame Ã§Ä±kar.\"\"\"\n    frame_path = f\"{WORK_DIR}/frame_{timestamp_sec}.jpg\"\n    !ffmpeg -ss {timestamp_sec} -i \"{video_path}\" -vframes 1 -q:v 2 \"{frame_path}\" -y -loglevel error\n    return frame_path if os.path.exists(frame_path) else None\n\ndef analyze_frame(image_path, product_hint):\n    \"\"\"LLaVA ile frame analiz et.\"\"\"\n    image = PILImage.open(image_path)\n    prompt = f\"Bu frame'de '{product_hint}' olarak bahsedilen Ã¼rÃ¼nÃ¼ tanÄ±yabilir misin? Marka ve model adÄ±nÄ± sÃ¶yle. TanÄ±yamÄ±yorsan 'UNKNOWN' de.\"\n    \n    conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]}]\n    formatted = vision_processor.apply_chat_template(conversation, add_generation_prompt=True)\n    inputs = vision_processor(images=image, text=formatted, return_tensors=\"pt\").to(vision_model.device)\n    output = vision_model.generate(**inputs, max_new_tokens=100, do_sample=False)\n    result = vision_processor.decode(output[0], skip_special_tokens=True)\n    if \"[/INST]\" in result:\n        result = result.split(\"[/INST]\")[-1].strip()\n    return result\n\n# Lost Ã¼rÃ¼nleri Ã§Ã¶z\nif lost and vision_model:\n    print(f\"\\n{len(lost)} lost Ã¼rÃ¼n Vision ile analiz ediliyor...\\n\")\n    resolved = []\n    \n    for product in lost:\n        ts = product.get(\"timestamp\", 0)\n        name = product.get(\"name\", \"Ã¼rÃ¼n\")\n        print(f\"ğŸ” '{name}' @ {ts}s ...\", end=\" \")\n        \n        frame_path = extract_frame(local_video_path, ts)\n        if frame_path:\n            guess = analyze_frame(frame_path, name)\n            print(f\"â†’ {guess[:50]}\")\n            \n            if guess and \"UNKNOWN\" not in guess.upper():\n                product[\"name\"] = guess.split(\"\\n\")[0][:100]  # Ä°lk satÄ±r, max 100 char\n                product[\"resolved_by\"] = \"vision\"\n                found.append(product)\n                resolved.append(product)\n    \n    # Resolved olanlarÄ± lost'tan Ã§Ä±kar\n    for r in resolved:\n        if r in lost:\n            lost.remove(r)\n    \n    print(f\"\\nâœ… {len(resolved)} Ã¼rÃ¼n Vision ile Ã§Ã¶zÃ¼ldÃ¼!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<cell_type>markdown</cell_type>---\n## 5. SonuÃ§lar"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*60)\nprint(\"SONUÃ‡LAR\")\nprint(\"=\"*60)\n\nprint(f\"\\nâœ… FOUND ({len(found)} Ã¼rÃ¼n):\")\nfor i, p in enumerate(found, 1):\n    resolved = \" [Vision]\" if p.get(\"resolved_by\") == \"vision\" else \"\"\n    print(f\"   {i}. {p['name']} ({p.get('category', '?')}){resolved}\")\n\nprint(f\"\\nâŒ LOST ({len(lost)} Ã¼rÃ¼n) - TanÄ±mlanamadÄ±:\")\nfor i, p in enumerate(lost, 1):\n    print(f\"   {i}. {p['name']} ({p.get('category', '?')})\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"Video indirildi mi: {'Evet' if local_video_path else 'HayÄ±r'}\")\nprint(f\"Transcript kaynaÄŸÄ±: {'YouTube' if not local_video_path else 'Whisper'}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. LLaVA ile Frame Analizi\n",
    "\n",
    "LLaVA - Open source vision-language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_frame_with_llava(image_path, prompt):\n",
    "    \"\"\"LLaVA ile frame analiz et.\"\"\"\n",
    "    image = PILImage.open(image_path)\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    formatted = vision_processor.apply_chat_template(\n",
    "        conversation, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = vision_processor(\n",
    "        images=image,\n",
    "        text=formatted,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(vision_model.device)\n",
    "    \n",
    "    output = vision_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    \n",
    "    result = vision_processor.decode(output[0], skip_special_tokens=True)\n",
    "    # Sadece cevabÄ± al\n",
    "    if \"[/INST]\" in result:\n",
    "        result = result.split(\"[/INST]\")[-1].strip()\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "if frames:\n",
    "    print(\"Test frame analizi...\")\n",
    "    display(Image(filename=frames[0], width=300))\n",
    "    \n",
    "    test_result = analyze_frame_with_llava(\n",
    "        frames[0],\n",
    "        \"What products or devices do you see in this image? List them briefly.\"\n",
    "    )\n",
    "    print(f\"\\nSonuÃ§: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. SonuÃ§larÄ± BirleÅŸtir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Performans ve Maliyet Ã–zeti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notlar ve DeÄŸerlendirme\n",
    "\n",
    "### Kalite DeÄŸerlendirmesi\n",
    "\n",
    "| Model | Kalite (/10) | Notlar |\n",
    "|-------|--------------|--------|\n",
    "| Whisper | /10 | |\n",
    "| Llama 3.1 | /10 | |\n",
    "| LLaVA | /10 | |\n",
    "\n",
    "### Bulunan ÃœrÃ¼nler DoÄŸru mu?\n",
    "- [ ] Evet, Ã§oÄŸu doÄŸru\n",
    "- [ ] YarÄ± yarÄ±ya\n",
    "- [ ] Ã‡oÄŸu yanlÄ±ÅŸ\n",
    "\n",
    "### KaÃ§an ÃœrÃ¼nler:\n",
    "- ...\n",
    "\n",
    "### YanlÄ±ÅŸ Tespitler:\n",
    "- ...\n",
    "\n",
    "### API vs Open Source:\n",
    "- [ ] Open source yeterli\n",
    "- [ ] API daha iyi ama open source kabul edilebilir\n",
    "- [ ] API ÅŸart, open source yetersiz\n",
    "\n",
    "### Sonraki AdÄ±mlar:\n",
    "- ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}