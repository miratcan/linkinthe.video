{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type># linkinthe.video - Open Source Pipeline\n\n**Akƒ±≈ü:**\n1. Transcript al (YouTube altyazƒ±sƒ± veya Whisper)\n2. LLM ile √ºr√ºn √ßƒ±kar ‚Üí found[] + lost[]\n3. Lost varsa ‚Üí Video indir ‚Üí Frame √ßƒ±kar ‚Üí Vision ile tanƒ±\n\n**Lazy Evaluation:** Video sadece gerektiƒüinde indirilir.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 0. Kurulum\n\n**√ñNEMLƒ∞:** Runtime ‚Üí Change runtime type ‚Üí T4 GPU se√ß!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GPU kontrol\n!nvidia-smi\n\nimport torch\nif torch.cuda.is_available():\n    print(f\"\\n‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"‚ùå GPU YOK! Runtime ‚Üí Change runtime type ‚Üí T4 GPU se√ß!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Paketleri kur\n!pip install -q youtube-transcript-api openai-whisper\n!pip install -q transformers accelerate bitsandbytes sentencepiece protobuf\n\nprint(\"‚úÖ Kurulum tamamlandƒ±!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport time\nimport requests\n\n# √áalƒ±≈üma klas√∂r√º\nWORK_DIR = \"/content/linkinthe_test\"\nos.makedirs(WORK_DIR, exist_ok=True)\n\n# Test videosu\nVIDEO_URL = \"https://www.youtube.com/watch?v=WlgjElhWD-U\"\nVIDEO_ID = VIDEO_URL.split(\"v=\")[-1].split(\"&\")[0]\n\n# State\nlocal_video_path = None\ntranscript_segments = None  # timestamp'li\ntranscript_text = None\n\nprint(f\"Video ID: {VIDEO_ID}\")\nprint(f\"Video URL: {VIDEO_URL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n## 1. Transcript Al"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from youtube_transcript_api import YouTubeTranscriptApi\n\n# √ñnce YouTube altyazƒ±sƒ±nƒ± dene\ntry:\n    transcript_segments = YouTubeTranscriptApi.get_transcript(VIDEO_ID, languages=['tr', 'en'])\n    transcript_text = \" \".join([s['text'] for s in transcript_segments])\n    print(f\"‚úÖ YouTube altyazƒ±sƒ± bulundu! ({len(transcript_text)} karakter)\")\n    print(f\"   {len(transcript_segments)} segment (timestamp'li)\")\n    print(\"\\n\" + \"=\"*60)\n    print(transcript_text[:1000])\n    print(\"=\"*60)\nexcept Exception as e:\n    print(f\"‚ùå YouTube altyazƒ±sƒ± yok: {e}\")\n    print(\"‚Üí Video indirip Whisper kullanacaƒüƒ±z...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n## 2. Video ƒ∞ndir + Whisper (sadece altyazƒ± yoksa)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def download_video(video_url, video_id):\n    \"\"\"Video indir, Drive'da cache'le.\"\"\"\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    persist_dir = \"/content/drive/MyDrive/linkinthe_cache\"\n    os.makedirs(persist_dir, exist_ok=True)\n    \n    persist_path = f\"{persist_dir}/{video_id}.mp4\"\n    local_path = f\"{WORK_DIR}/video.mp4\"\n    \n    if os.path.exists(persist_path):\n        size_mb = os.path.getsize(persist_path) / (1024 * 1024)\n        print(f\"‚úÖ Video cache'de: {size_mb:.1f} MB\")\n        !cp \"{persist_path}\" \"{local_path}\"\n        return local_path\n    \n    print(f\"Video indiriliyor...\")\n    RAPIDAPI_KEY = \"5b27c409a5msh591a62b8591e8d8p156a0bjsn977cc2fc9567\"\n    \n    resp = requests.post(\n        \"https://yt-video-audio-downloader-api.p.rapidapi.com/download\",\n        headers={\n            \"Content-Type\": \"application/json\",\n            \"x-rapidapi-host\": \"yt-video-audio-downloader-api.p.rapidapi.com\",\n            \"x-rapidapi-key\": RAPIDAPI_KEY,\n        },\n        json={\"url\": video_url, \"format\": \"mp4\", \"quality\": 480}\n    )\n    \n    if resp.status_code == 200:\n        data = resp.json()\n        download_url = data.get(\"download_url\") or data.get(\"url\") or data.get(\"link\")\n        if download_url:\n            video_resp = requests.get(download_url, stream=True)\n            with open(local_path, \"wb\") as f:\n                for chunk in video_resp.iter_content(chunk_size=8192):\n                    f.write(chunk)\n            size_mb = os.path.getsize(local_path) / (1024 * 1024)\n            print(f\"‚úÖ Video indirildi: {size_mb:.1f} MB\")\n            !cp \"{local_path}\" \"{persist_path}\"\n            return local_path\n        else:\n            print(f\"‚ùå Download URL bulunamadƒ±: {data}\")\n    else:\n        print(f\"‚ùå API Hatasƒ±: {resp.status_code}\")\n    return None\n\n# Transcript yoksa ‚Üí indir + Whisper\nif not transcript_text:\n    local_video_path = download_video(VIDEO_URL, VIDEO_ID)\n    \n    if local_video_path:\n        # Ses √ßƒ±kar\n        audio_path = f\"{WORK_DIR}/audio.wav\"\n        !ffmpeg -i \"{local_video_path}\" -vn -acodec pcm_s16le -ar 16000 -ac 1 \"{audio_path}\" -y -loglevel error\n        print(f\"‚úÖ Ses √ßƒ±karƒ±ldƒ±\")\n        \n        # Whisper\n        import whisper\n        print(\"Whisper y√ºkleniyor...\")\n        model = whisper.load_model(\"medium\")\n        \n        print(\"Transkript alƒ±nƒ±yor...\")\n        result = model.transcribe(audio_path)\n        \n        transcript_text = result['text']\n        transcript_segments = result['segments']  # timestamp'li\n        print(f\"‚úÖ Whisper tamamlandƒ±! ({len(transcript_text)} karakter)\")\n        \n        del model\n        torch.cuda.empty_cache()\nelse:\n    print(\"‚è≠Ô∏è YouTube altyazƒ±sƒ± var, video indirmeye gerek yok.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n## 3. LLM ile √úr√ºn √áƒ±kar (found/lost)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nMODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\nprint(\"Llama 3.1 8B y√ºkleniyor...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\nllm_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\nprint(\"‚úÖ Llama y√ºklendi!\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def ask_llama(prompt, max_tokens=1500):\n    messages = [\n        {\"role\": \"system\", \"content\": \"Sen bir √ºr√ºn tespit asistanƒ±sƒ±n. JSON formatƒ±nda cevap ver.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(llm_model.device)\n    outputs = llm_model.generate(input_ids, max_new_tokens=max_tokens, do_sample=True, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n\n# √úr√ºn √ßƒ±karma prompt'u - found/lost ayƒ±rƒ±yor\nPRODUCT_PROMPT = f\"\"\"Bu bir YouTube videosunun transkripti. Videoda bahsedilen √ºr√ºnleri bul.\n\nKURALLAR:\n1. Emin olduƒüun √ºr√ºnler (marka+model belli) ‚Üí \"found\" listesine\n2. Belirsiz olanlar (sadece \"kablo\", \"tripod\" gibi) ‚Üí \"lost\" listesine  \n3. Duplicate ekleme, aynƒ± √ºr√ºn√º bir kere yaz\n4. Her √ºr√ºn i√ßin timestamp (saniye) ver\n\nJSON formatƒ±:\n{{\n  \"found\": [{{\"name\": \"Sony A7IV\", \"category\": \"kamera\", \"timestamp\": 125}}],\n  \"lost\": [{{\"name\": \"kablo\", \"category\": \"aksesuar\", \"timestamp\": 340}}]\n}}\n\nTranskript:\n---\n{transcript_text[:6000]}\n---\n\nSADECE JSON d√∂n.\"\"\"\n\nprint(\"√úr√ºnler √ßƒ±karƒ±lƒ±yor...\")\nstart = time.time()\nllm_response = ask_llama(PRODUCT_PROMPT)\nprint(f\"‚úÖ Tamamlandƒ±! ({time.time()-start:.1f} sn)\")\nprint(llm_response)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# JSON parse et\ndef parse_llm_json(text):\n    clean = text\n    if \"```json\" in clean:\n        clean = clean.split(\"```json\")[1].split(\"```\")[0]\n    elif \"```\" in clean:\n        clean = clean.split(\"```\")[1].split(\"```\")[0]\n    start_idx = clean.find(\"{\")\n    end_idx = clean.rfind(\"}\") + 1\n    if start_idx != -1 and end_idx > start_idx:\n        clean = clean[start_idx:end_idx]\n    return json.loads(clean)\n\ntry:\n    data = parse_llm_json(llm_response)\n    found = data.get(\"found\", [])\n    lost = data.get(\"lost\", [])\n    \n    print(f\"‚úÖ FOUND ({len(found)} √ºr√ºn):\")\n    for p in found:\n        print(f\"   üü¢ {p['name']} ({p.get('category', '?')}) @ {p.get('timestamp', '?')}s\")\n    \n    print(f\"\\n‚ö†Ô∏è LOST ({len(lost)} √ºr√ºn) - Vision ile √ß√∂z√ºlecek:\")\n    for p in lost:\n        print(f\"   üü° {p['name']} ({p.get('category', '?')}) @ {p.get('timestamp', '?')}s\")\nexcept Exception as e:\n    print(f\"‚ùå Parse hatasƒ±: {e}\")\n    found, lost = [], []\n\n# Llama'yƒ± temizle\ndel llm_model, tokenizer\ntorch.cuda.empty_cache()\nprint(\"\\n‚úÖ Llama bellekten silindi.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<cell_type>markdown</cell_type>---\n## 4. Lost √úr√ºnleri Vision ile √á√∂z\n\nVideo sadece lost varsa indirilir."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not lost:\n    print(\"‚úÖ Lost yok, Vision'a gerek yok!\")\n    vision_model = None\nelse:\n    # Video indir (hen√ºz indirilmediyse)\n    if not local_video_path:\n        print(\"Lost var, video indiriliyor...\")\n        local_video_path = download_video(VIDEO_URL, VIDEO_ID)\n    \n    if local_video_path:\n        # LLaVA y√ºkle\n        from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n        from PIL import Image as PILImage\n        \n        print(\"LLaVA y√ºkleniyor...\")\n        VISION_MODEL = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n        vision_processor = LlavaNextProcessor.from_pretrained(VISION_MODEL)\n        vision_model = LlavaNextForConditionalGeneration.from_pretrained(\n            VISION_MODEL,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n            load_in_4bit=True,\n        )\n        print(\"‚úÖ LLaVA y√ºklendi!\")\n    else:\n        print(\"‚ùå Video indirilemedi, Vision atlanƒ±yor\")\n        vision_model = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_frame(video_path, timestamp_sec):\n    \"\"\"Video'dan belirli saniyede frame √ßƒ±kar.\"\"\"\n    frame_path = f\"{WORK_DIR}/frame_{timestamp_sec}.jpg\"\n    !ffmpeg -ss {timestamp_sec} -i \"{video_path}\" -vframes 1 -q:v 2 \"{frame_path}\" -y -loglevel error\n    return frame_path if os.path.exists(frame_path) else None\n\ndef analyze_frame(image_path, product_hint):\n    \"\"\"LLaVA ile frame analiz et.\"\"\"\n    image = PILImage.open(image_path)\n    prompt = f\"Bu frame'de '{product_hint}' olarak bahsedilen √ºr√ºn√º tanƒ±yabilir misin? Marka ve model adƒ±nƒ± s√∂yle. Tanƒ±yamƒ±yorsan 'UNKNOWN' de.\"\n    \n    conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]}]\n    formatted = vision_processor.apply_chat_template(conversation, add_generation_prompt=True)\n    inputs = vision_processor(images=image, text=formatted, return_tensors=\"pt\").to(vision_model.device)\n    output = vision_model.generate(**inputs, max_new_tokens=100, do_sample=False)\n    result = vision_processor.decode(output[0], skip_special_tokens=True)\n    if \"[/INST]\" in result:\n        result = result.split(\"[/INST]\")[-1].strip()\n    return result\n\n# Lost √ºr√ºnleri √ß√∂z\nif lost and vision_model:\n    print(f\"\\n{len(lost)} lost √ºr√ºn Vision ile analiz ediliyor...\\n\")\n    resolved = []\n    \n    for product in lost:\n        ts = product.get(\"timestamp\", 0)\n        name = product.get(\"name\", \"√ºr√ºn\")\n        print(f\"üîç '{name}' @ {ts}s ...\", end=\" \")\n        \n        frame_path = extract_frame(local_video_path, ts)\n        if frame_path:\n            guess = analyze_frame(frame_path, name)\n            print(f\"‚Üí {guess[:50]}\")\n            \n            if guess and \"UNKNOWN\" not in guess.upper():\n                product[\"name\"] = guess.split(\"\\n\")[0][:100]  # ƒ∞lk satƒ±r, max 100 char\n                product[\"resolved_by\"] = \"vision\"\n                found.append(product)\n                resolved.append(product)\n    \n    # Resolved olanlarƒ± lost'tan √ßƒ±kar\n    for r in resolved:\n        if r in lost:\n            lost.remove(r)\n    \n    print(f\"\\n‚úÖ {len(resolved)} √ºr√ºn Vision ile √ß√∂z√ºld√º!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<cell_type>markdown</cell_type>---\n## 5. Sonu√ßlar"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*60)\nprint(\"SONU√áLAR\")\nprint(\"=\"*60)\n\nprint(f\"\\n‚úÖ FOUND ({len(found)} √ºr√ºn):\")\nfor i, p in enumerate(found, 1):\n    resolved = \" [Vision]\" if p.get(\"resolved_by\") == \"vision\" else \"\"\n    print(f\"   {i}. {p['name']} ({p.get('category', '?')}){resolved}\")\n\nprint(f\"\\n‚ùå LOST ({len(lost)} √ºr√ºn) - Tanƒ±mlanamadƒ±:\")\nfor i, p in enumerate(lost, 1):\n    print(f\"   {i}. {p['name']} ({p.get('category', '?')})\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"Video indirildi mi: {'Evet' if local_video_path else 'Hayƒ±r'}\")\nprint(f\"Transcript kaynaƒüƒ±: {'YouTube' if not local_video_path else 'Whisper'}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. LLaVA ile Frame Analizi\n",
    "\n",
    "LLaVA - Open source vision-language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_frame_with_llava(image_path, prompt):\n",
    "    \"\"\"LLaVA ile frame analiz et.\"\"\"\n",
    "    image = PILImage.open(image_path)\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    formatted = vision_processor.apply_chat_template(\n",
    "        conversation, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = vision_processor(\n",
    "        images=image,\n",
    "        text=formatted,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(vision_model.device)\n",
    "    \n",
    "    output = vision_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    \n",
    "    result = vision_processor.decode(output[0], skip_special_tokens=True)\n",
    "    # Sadece cevabƒ± al\n",
    "    if \"[/INST]\" in result:\n",
    "        result = result.split(\"[/INST]\")[-1].strip()\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "if frames:\n",
    "    print(\"Test frame analizi...\")\n",
    "    display(Image(filename=frames[0], width=300))\n",
    "    \n",
    "    test_result = analyze_frame_with_llava(\n",
    "        frames[0],\n",
    "        \"What products or devices do you see in this image? List them briefly.\"\n",
    "    )\n",
    "    print(f\"\\nSonu√ß: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Sonu√ßlarƒ± Birle≈ütir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Performans ve Maliyet √ñzeti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notlar ve Deƒüerlendirme\n",
    "\n",
    "### Kalite Deƒüerlendirmesi\n",
    "\n",
    "| Model | Kalite (/10) | Notlar |\n",
    "|-------|--------------|--------|\n",
    "| Whisper | /10 | |\n",
    "| Llama 3.1 | /10 | |\n",
    "| LLaVA | /10 | |\n",
    "\n",
    "### Bulunan √úr√ºnler Doƒüru mu?\n",
    "- [ ] Evet, √ßoƒüu doƒüru\n",
    "- [ ] Yarƒ± yarƒ±ya\n",
    "- [ ] √áoƒüu yanlƒ±≈ü\n",
    "\n",
    "### Ka√ßan √úr√ºnler:\n",
    "- ...\n",
    "\n",
    "### Yanlƒ±≈ü Tespitler:\n",
    "- ...\n",
    "\n",
    "### API vs Open Source:\n",
    "- [ ] Open source yeterli\n",
    "- [ ] API daha iyi ama open source kabul edilebilir\n",
    "- [ ] API ≈üart, open source yetersiz\n",
    "\n",
    "### Sonraki Adƒ±mlar:\n",
    "- ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}