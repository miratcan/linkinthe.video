{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# linkinthe.video - Open Source Pipeline\n\n**Modeller:**\n- Whisper - Transcription (fallback)\n- Mistral 7B - √úr√ºn √ßƒ±karma\n- LLaVA 1.6 - Frame analizi\n\n**Akƒ±≈ü:**\n1. Transcript al (YouTube altyazƒ±sƒ± veya Whisper)\n2. LLM ile √ºr√ºn √ßƒ±kar ‚Üí found[] + lost[]\n3. Lost varsa ‚Üí Video indir ‚Üí Frame √ßƒ±kar ‚Üí Vision ile tanƒ±\n\n**Lazy Evaluation:** Video sadece gerektiƒüinde indirilir.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 0. Kurulum\n\n**√ñNEMLƒ∞:** Runtime ‚Üí Change runtime type ‚Üí T4 GPU se√ß!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GPU kontrol\n!nvidia-smi\n\nimport torch\nif torch.cuda.is_available():\n    print(f\"\\n‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"‚ùå GPU YOK! Runtime ‚Üí Change runtime type ‚Üí T4 GPU se√ß!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Paketleri kur\n!pip install -q youtube-transcript-api openai-whisper\n!pip install -q transformers accelerate bitsandbytes sentencepiece protobuf\n\nprint(\"‚úÖ Kurulum tamamlandƒ±!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport time\nimport requests\n\n# √áalƒ±≈üma klas√∂r√º\nWORK_DIR = \"/content/linkinthe_test\"\nos.makedirs(WORK_DIR, exist_ok=True)\n\n# Test videosu\nVIDEO_URL = \"https://www.youtube.com/watch?v=WlgjElhWD-U\"\nVIDEO_ID = VIDEO_URL.split(\"v=\")[-1].split(\"&\")[0]\n\n# State\nlocal_video_path = None\ntranscript_segments = None  # timestamp'li\ntranscript_text = None\n\nprint(f\"Video ID: {VIDEO_ID}\")\nprint(f\"Video URL: {VIDEO_URL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n## 1. Transcript Al"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from youtube_transcript_api import YouTubeTranscriptApi\n\n# √ñnce YouTube altyazƒ±sƒ±nƒ± dene\ntry:\n    transcript_segments = YouTubeTranscriptApi.get_transcript(VIDEO_ID, languages=['tr', 'en'])\n    \n    # Timestamp'li format: \"[0:35] text\"\n    def format_timestamp(seconds):\n        m, s = divmod(int(seconds), 60)\n        return f\"{m}:{s:02d}\"\n    \n    transcript_with_ts = \"\\n\".join([\n        f\"[{format_timestamp(s['start'])}] {s['text']}\" \n        for s in transcript_segments\n    ])\n    transcript_text = \" \".join([s['text'] for s in transcript_segments])\n    \n    print(f\"‚úÖ YouTube altyazƒ±sƒ± bulundu! ({len(transcript_segments)} segment)\")\n    print(\"\\n\" + \"=\"*60)\n    print(transcript_with_ts[:2000])\n    print(\"=\"*60)\nexcept Exception as e:\n    print(f\"‚ùå YouTube altyazƒ±sƒ± yok: {e}\")\n    print(\"‚Üí Video y√ºkle ve Whisper kullan\")\n    transcript_segments = None\n    transcript_with_ts = None\n    transcript_text = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n## 2. Video (sadece altyazƒ± yoksa)\n\nYouTube altyazƒ±sƒ± varsa bu adƒ±mƒ± **ATLA**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Video dosyasƒ± - Drive'a koy veya Colab'a y√ºkle\nlocal_video_path = f\"{WORK_DIR}/sample.webm\"\n\nif os.path.exists(local_video_path):\n    size_mb = os.path.getsize(local_video_path) / (1024 * 1024)\n    print(f\"‚úÖ Video mevcut: {local_video_path} ({size_mb:.1f} MB)\")\nelse:\n    print(f\"‚ùå Video bulunamadƒ±: {local_video_path}\")\n    print(\"   Drive'dan kopyala veya y√ºkle\")"
  },
  {
   "cell_type": "code",
   "source": "# Transcript yoksa ‚Üí indir + Whisper\nif not transcript_text:\n    local_video_path = download_video(VIDEO_URL, VIDEO_ID)\n    \n    if local_video_path:\n        # Ses √ßƒ±kar\n        audio_path = f\"{WORK_DIR}/audio.wav\"\n        !ffmpeg -i \"{local_video_path}\" -vn -acodec pcm_s16le -ar 16000 -ac 1 \"{audio_path}\" -y -loglevel error\n        print(f\"‚úÖ Ses √ßƒ±karƒ±ldƒ±\")\n        \n        # Whisper\n        import whisper\n        print(\"Whisper y√ºkleniyor...\")\n        model = whisper.load_model(\"medium\")\n        \n        print(\"Transkript alƒ±nƒ±yor...\")\n        result = model.transcribe(audio_path)\n        \n        transcript_text = result['text']\n        transcript_segments = result['segments']\n        print(f\"‚úÖ Whisper tamamlandƒ±! ({len(transcript_text)} karakter)\")\n        \n        del model\n        torch.cuda.empty_cache()\nelse:\n    print(\"‚è≠Ô∏è YouTube altyazƒ±sƒ± var, video indirmeye gerek yok.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n## 3. LLM ile √úr√ºn √áƒ±kar (found/lost)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Mistral 7B - iyi instruction following\nMODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n\nprint(\"Mistral 7B y√ºkleniyor...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\nllm_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n)\nprint(\"‚úÖ Mistral y√ºklendi!\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def ask_llm(prompt, max_tokens=1500):\n    \"\"\"Mistral ile soru sor.\"\"\"\n    # Mistral system role desteklemiyor, sadece user\n    messages = [\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(llm_model.device)\n    outputs = llm_model.generate(input_ids, max_new_tokens=max_tokens, do_sample=True, temperature=0.1, pad_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n\n# √úr√ºn √ßƒ±karma prompt'u - sƒ±kƒ± kurallar\nPRODUCT_PROMPT = f\"\"\"Bu bir YouTube videosunun transkripti. Videoda bahsedilen √ºr√ºnleri bul.\n\n== FOUND (SADECE marka/model varsa) ==\nFOUND'a koymak i√ßin MARKA ≈ûART:\n‚úì \"Logitech G Pro X\" ‚Üí FOUND (marka + model var)\n‚úì \"Nintendo Switch\" ‚Üí FOUND (marka + √ºr√ºn adƒ± var)\n‚úì \"iPad 8th generation\" ‚Üí FOUND (marka + nesil var)\n‚úì \"Funko Pop Yoda\" ‚Üí FOUND (marka + karakter var)\n\n== LOST (marka/model YOK) ==\nMarka yoksa LOST'a koy, istisnasƒ±z:\n‚úó \"gaming mouse\" ‚Üí LOST (hangi marka?)\n‚úó \"tripod\" ‚Üí LOST (hangi tripod?)\n‚úó \"headset\" ‚Üí LOST (marka yok)\n‚úó \"wireless earbuds\" ‚Üí LOST (model yok)\n‚úó \"stress ball\" ‚Üí LOST (marka yok)\n‚úó \"water bottle\" ‚Üí LOST (marka yok)\n\n== Hƒ∞√á EKLEME ==\n- Fiyatlar: $100, \"150 dollar item\"\n- Yerler: Japan, Tokyo, Amazon\n- Kutular: box, mystery box, returns box\n- Soyut kavramlar: disadvantage, stumbling block\n- Servisler: YouTube, Underdog\n- ƒ∞nsanlar: isimler, YouTuber'lar\n\nKURAL: Marka yoksa ‚Üí LOST. ƒ∞stisna yok.\n\nJSON formatƒ±:\n{{\"found\": [{{\"name\": \"...\", \"category\": \"...\", \"timestamp\": \"...\"}}], \"lost\": [...]}}\n\nTranskript:\n---\n{transcript_with_ts[:6000] if transcript_with_ts else transcript_text[:6000]}\n---\n\nSADECE JSON d√∂n.\"\"\"\n\nprint(\"√úr√ºnler √ßƒ±karƒ±lƒ±yor...\")\nstart = time.time()\nllm_response = ask_llm(PRODUCT_PROMPT)\nprint(f\"‚úÖ Tamamlandƒ±! ({time.time()-start:.1f} sn)\")\nprint(llm_response)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# JSON parse et\ndef parse_llm_json(text):\n    clean = text\n    if \"```json\" in clean:\n        clean = clean.split(\"```json\")[1].split(\"```\")[0]\n    elif \"```\" in clean:\n        clean = clean.split(\"```\")[1].split(\"```\")[0]\n    start_idx = clean.find(\"{\")\n    end_idx = clean.rfind(\"}\") + 1\n    if start_idx != -1 and end_idx > start_idx:\n        clean = clean[start_idx:end_idx]\n    return json.loads(clean)\n\ntry:\n    data = parse_llm_json(llm_response)\n    found = data.get(\"found\", [])\n    lost = data.get(\"lost\", [])\n    \n    print(f\"‚úÖ FOUND ({len(found)} √ºr√ºn):\")\n    for p in found:\n        print(f\"   üü¢ {p['name']} ({p.get('category', '?')}) @ {p.get('timestamp', '?')}s\")\n    \n    print(f\"\\n‚ö†Ô∏è LOST ({len(lost)} √ºr√ºn) - Vision ile √ß√∂z√ºlecek:\")\n    for p in lost:\n        print(f\"   üü° {p['name']} ({p.get('category', '?')}) @ {p.get('timestamp', '?')}s\")\nexcept Exception as e:\n    print(f\"‚ùå Parse hatasƒ±: {e}\")\n    found, lost = [], []\n\n# LLM'i temizle\ndel llm_model, tokenizer\ntorch.cuda.empty_cache()\nprint(\"\\n‚úÖ Mistral bellekten silindi.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<cell_type>markdown</cell_type>---\n## 4. Lost √úr√ºnleri Vision ile √á√∂z\n\nVideo sadece lost varsa indirilir."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not lost:\n    print(\"‚úÖ Lost yok, Vision'a gerek yok!\")\n    vision_model = None\nelse:\n    # Video indir (hen√ºz indirilmediyse)\n    if not local_video_path:\n        print(\"Lost var, video indiriliyor...\")\n        local_video_path = download_video(VIDEO_URL, VIDEO_ID)\n    \n    if local_video_path:\n        # LLaVA y√ºkle\n        from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n        from PIL import Image as PILImage\n        \n        print(\"LLaVA y√ºkleniyor...\")\n        VISION_MODEL = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n        vision_processor = LlavaNextProcessor.from_pretrained(VISION_MODEL)\n        vision_model = LlavaNextForConditionalGeneration.from_pretrained(\n            VISION_MODEL,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n            load_in_4bit=True,\n        )\n        print(\"‚úÖ LLaVA y√ºklendi!\")\n    else:\n        print(\"‚ùå Video indirilemedi, Vision atlanƒ±yor\")\n        vision_model = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_frame(video_path, timestamp_sec):\n    \"\"\"Video'dan belirli saniyede frame √ßƒ±kar.\"\"\"\n    frame_path = f\"{WORK_DIR}/frame_{timestamp_sec}.jpg\"\n    !ffmpeg -ss {timestamp_sec} -i \"{video_path}\" -vframes 1 -q:v 2 \"{frame_path}\" -y -loglevel error\n    return frame_path if os.path.exists(frame_path) else None\n\ndef analyze_frame(image_path, product_hint):\n    \"\"\"LLaVA ile frame analiz et.\"\"\"\n    image = PILImage.open(image_path)\n    prompt = f\"Bu frame'de '{product_hint}' olarak bahsedilen √ºr√ºn√º tanƒ±yabilir misin? Marka ve model adƒ±nƒ± s√∂yle. Tanƒ±yamƒ±yorsan 'UNKNOWN' de.\"\n    \n    conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]}]\n    formatted = vision_processor.apply_chat_template(conversation, add_generation_prompt=True)\n    inputs = vision_processor(images=image, text=formatted, return_tensors=\"pt\").to(vision_model.device)\n    output = vision_model.generate(**inputs, max_new_tokens=100, do_sample=False)\n    result = vision_processor.decode(output[0], skip_special_tokens=True)\n    if \"[/INST]\" in result:\n        result = result.split(\"[/INST]\")[-1].strip()\n    return result\n\n# Lost √ºr√ºnleri √ß√∂z\nif lost and vision_model:\n    print(f\"\\n{len(lost)} lost √ºr√ºn Vision ile analiz ediliyor...\\n\")\n    resolved = []\n    \n    for product in lost:\n        ts = product.get(\"timestamp\", 0)\n        name = product.get(\"name\", \"√ºr√ºn\")\n        print(f\"üîç '{name}' @ {ts}s ...\", end=\" \")\n        \n        frame_path = extract_frame(local_video_path, ts)\n        if frame_path:\n            guess = analyze_frame(frame_path, name)\n            print(f\"‚Üí {guess[:50]}\")\n            \n            if guess and \"UNKNOWN\" not in guess.upper():\n                product[\"name\"] = guess.split(\"\\n\")[0][:100]  # ƒ∞lk satƒ±r, max 100 char\n                product[\"resolved_by\"] = \"vision\"\n                found.append(product)\n                resolved.append(product)\n    \n    # Resolved olanlarƒ± lost'tan √ßƒ±kar\n    for r in resolved:\n        if r in lost:\n            lost.remove(r)\n    \n    print(f\"\\n‚úÖ {len(resolved)} √ºr√ºn Vision ile √ß√∂z√ºld√º!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "<cell_type>markdown</cell_type>---\n## 5. Sonu√ßlar"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*60)\nprint(\"SONU√áLAR\")\nprint(\"=\"*60)\n\nprint(f\"\\n‚úÖ FOUND ({len(found)} √ºr√ºn):\")\nfor i, p in enumerate(found, 1):\n    resolved = \" [Vision]\" if p.get(\"resolved_by\") == \"vision\" else \"\"\n    print(f\"   {i}. {p['name']} ({p.get('category', '?')}){resolved}\")\n\nprint(f\"\\n‚ùå LOST ({len(lost)} √ºr√ºn) - Tanƒ±mlanamadƒ±:\")\nfor i, p in enumerate(lost, 1):\n    print(f\"   {i}. {p['name']} ({p.get('category', '?')})\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"Video indirildi mi: {'Evet' if local_video_path else 'Hayƒ±r'}\")\nprint(f\"Transcript kaynaƒüƒ±: {'YouTube' if not local_video_path else 'Whisper'}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. LLaVA ile Frame Analizi\n",
    "\n",
    "LLaVA - Open source vision-language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_frame_with_llava(image_path, prompt):\n",
    "    \"\"\"LLaVA ile frame analiz et.\"\"\"\n",
    "    image = PILImage.open(image_path)\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    formatted = vision_processor.apply_chat_template(\n",
    "        conversation, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = vision_processor(\n",
    "        images=image,\n",
    "        text=formatted,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(vision_model.device)\n",
    "    \n",
    "    output = vision_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    \n",
    "    result = vision_processor.decode(output[0], skip_special_tokens=True)\n",
    "    # Sadece cevabƒ± al\n",
    "    if \"[/INST]\" in result:\n",
    "        result = result.split(\"[/INST]\")[-1].strip()\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "if frames:\n",
    "    print(\"Test frame analizi...\")\n",
    "    display(Image(filename=frames[0], width=300))\n",
    "    \n",
    "    test_result = analyze_frame_with_llava(\n",
    "        frames[0],\n",
    "        \"What products or devices do you see in this image? List them briefly.\"\n",
    "    )\n",
    "    print(f\"\\nSonu√ß: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Sonu√ßlarƒ± Birle≈ütir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Performans ve Maliyet √ñzeti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notlar ve Deƒüerlendirme\n",
    "\n",
    "### Kalite Deƒüerlendirmesi\n",
    "\n",
    "| Model | Kalite (/10) | Notlar |\n",
    "|-------|--------------|--------|\n",
    "| Whisper | /10 | |\n",
    "| Llama 3.1 | /10 | |\n",
    "| LLaVA | /10 | |\n",
    "\n",
    "### Bulunan √úr√ºnler Doƒüru mu?\n",
    "- [ ] Evet, √ßoƒüu doƒüru\n",
    "- [ ] Yarƒ± yarƒ±ya\n",
    "- [ ] √áoƒüu yanlƒ±≈ü\n",
    "\n",
    "### Ka√ßan √úr√ºnler:\n",
    "- ...\n",
    "\n",
    "### Yanlƒ±≈ü Tespitler:\n",
    "- ...\n",
    "\n",
    "### API vs Open Source:\n",
    "- [ ] Open source yeterli\n",
    "- [ ] API daha iyi ama open source kabul edilebilir\n",
    "- [ ] API ≈üart, open source yetersiz\n",
    "\n",
    "### Sonraki Adƒ±mlar:\n",
    "- ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}